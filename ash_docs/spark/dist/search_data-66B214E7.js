searchData={"items":[{"type":"behaviour","title":"Spark.Dsl","doc":"The primary entry point for adding a DSL to a module.\n\nTo add a DSL to a module, add `use Spark.Dsl, ...options`. The options supported with `use Spark.Dsl` are:\n\n* `:single_extension_kinds` (list of `t:atom/0`) - The extension kinds that are allowed to have a single value. For example: `[:data_layer]` The default value is `[]`.\n\n* `:many_extension_kinds` (list of `t:atom/0`) - The extension kinds that can have multiple values. e.g `[notifiers: [Notifier1, Notifier2]]` The default value is `[]`.\n\n* `:untyped_extensions?` (`t:boolean/0`) - Whether or not to support an `extensions` key which contains untyped extensions The default value is `true`.\n\n* `:extension_kind_types` (`t:keyword/0`) - A keyword list of extension kinds and their types, e.g `[authorizers: {:list, {:behaviour, Ash.Authorizer}}]` The default value is `[]`.\n\n* `:extension_kind_docs` (`t:keyword/0`) - A keyword list of extension kinds and a short documentation snippet to be used when autocompleting that option The default value is `[]`.\n\n* `:default_extensions` (`t:keyword/0`) - The extensions that are included by default. e.g `[data_layer: Default, notifiers: [Notifier1]]`\n  Default values for single extension kinds are overwritten if specified by the implementor, while many extension\n  kinds are appended to if specified by the implementor. The default value is `[]`.\n\n* `:opt_schema` (`t:keyword/0`) - A schema for additional options to accept when calling `use YourSpark` The default value is `[]`.\n\n* `:opts_to_document` - A list of `t:atom/0` or `:all`. Spark automatically detects options and documents them in `@moduledoc`.\n  You can instruct Spark to use only a subset of options, e.g. `opts_to_document: [:fragments]`. The default value is `:all`.\n\n\n\nSee the callbacks defined in this module to augment the behavior/compilation of the module getting a Dsl.","ref":"Spark.Dsl.html"},{"type":"behaviour","title":"Schemas/Data Types - Spark.Dsl","doc":"For more information, see `Spark.Options`.","ref":"Spark.Dsl.html#module-schemas-data-types"},{"type":"callback","title":"Spark.Dsl.explain/2","doc":"Validate/add options. Those options will be passed to `handle_opts` and `handle_before_compile`","ref":"Spark.Dsl.html#c:explain/2"},{"type":"callback","title":"Spark.Dsl.handle_before_compile/1","doc":"Handle options in the context of the module, after all extensions have been processed. Must return a `quote` block.","ref":"Spark.Dsl.html#c:handle_before_compile/1"},{"type":"function","title":"Spark.Dsl.handle_fragments/2","doc":"","ref":"Spark.Dsl.html#handle_fragments/2"},{"type":"callback","title":"Spark.Dsl.handle_opts/1","doc":"Handle options in the context of the module. Must return a `quote` block.\n\nIf you want to persist anything in the DSL persistence layer,\nuse `@persist {:key, value}`. It can be called multiple times to\npersist multiple times.","ref":"Spark.Dsl.html#c:handle_opts/1"},{"type":"callback","title":"Spark.Dsl.init/1","doc":"Validate/add options. Those options will be passed to `handle_opts` and `handle_before_compile`","ref":"Spark.Dsl.html#c:init/1"},{"type":"function","title":"Spark.Dsl.is?/2","doc":"","ref":"Spark.Dsl.html#is?/2"},{"type":"callback","title":"Spark.Dsl.verify/2","doc":"A callback that is called in the `after_verify` hook. Only runs on versions of Elixir >= 1.14.0","ref":"Spark.Dsl.html#c:verify/2"},{"type":"type","title":"Spark.Dsl.entity/0","doc":"","ref":"Spark.Dsl.html#t:entity/0"},{"type":"type","title":"Spark.Dsl.opts/0","doc":"","ref":"Spark.Dsl.html#t:opts/0"},{"type":"type","title":"Spark.Dsl.section/0","doc":"","ref":"Spark.Dsl.html#t:section/0"},{"type":"type","title":"Spark.Dsl.t/0","doc":"","ref":"Spark.Dsl.html#t:t/0"},{"type":"module","title":"Spark.Dsl.Builder","doc":"Utilities for building DSL objects programatically, generally used in transformers.","ref":"Spark.Dsl.Builder.html"},{"type":"macro","title":"Spark.Dsl.Builder.defbuilder/2","doc":"","ref":"Spark.Dsl.Builder.html#defbuilder/2"},{"type":"macro","title":"Spark.Dsl.Builder.defbuilderp/2","doc":"","ref":"Spark.Dsl.Builder.html#defbuilderp/2"},{"type":"function","title":"Spark.Dsl.Builder.handle_nested_builders/2","doc":"Handles nested values that may be `{:ok, result}` or `{:error, term}`, returning any errors and unwrapping any ok values\n\nThis allows users of builders to do things like:\n\n```elixir\ndsl_state\n|> Ash.Resource.Builder.add_new_action(:update, :publish,\n  changes: [\n    Ash.Resource.Builder.build_action_change(\n      Ash.Resource.Change.Builtins.set_attribute(:state, :published)\n    )\n  ]\n)\n```\n\nIf your builder function calls `handle_nested_builders/2` with their input before building the thing its building.","ref":"Spark.Dsl.Builder.html#handle_nested_builders/2"},{"type":"type","title":"Spark.Dsl.Builder.input/0","doc":"","ref":"Spark.Dsl.Builder.html#t:input/0"},{"type":"type","title":"Spark.Dsl.Builder.result/0","doc":"","ref":"Spark.Dsl.Builder.html#t:result/0"},{"type":"module","title":"Spark.Dsl.Entity","doc":"Declares a DSL entity.\n\nA dsl entity represents a dsl constructor who's resulting value is a struct.\nThis lets the user create complex objects with arbitrary(mostly) validation rules.\n\nThe lifecycle of creating entities is complex, happening as Elixir is compiling\nthe modules in question. Some of the patterns around validating/transforming entities\nhave not yet solidified. If you aren't careful and don't follow the guidelines listed\nhere, you can have subtle and strange bugs during compilation. Anything not isolated to\nsimple value validations should be done in `transformers`. See `Spark.Dsl.Transformer`.\n\nAn entity has a `target` indicating which struct will ultimately be built. An entity\nalso has a `schema`. This schema is used for documentation, and the options are validated\nagainst it before continuing on with the DSL.\n\nTo create positional arguments to the builder, use `args`. The values provided to\n`args` need to be in the provided schema as well. They will be positional arguments\nin the same order that they are provided in the `args` key.\n\n`auto_set_fields` will set the provided values into the produced struct (they do not need\nto be included in the schema).\n\n`transform` is a function that takes a created struct and can alter it. This happens immediately\nafter handling the DSL options, and can be useful for setting field values on a struct based on\nother values in that struct. If you need things that aren't contained in that struct, use an\n`Spark.Dsl.Transformer`. This function returns `{:ok, new_entity}` or `{:error, error}`, so this can\nalso be used to validate the entity.\n\n`entities` allows you to specify a keyword list of nested entities. Nested entities are stored\non the struct in the corresponding key, and are used in the same way entities are otherwise.\n\n`singleton_entity_keys` specifies a set of entity keys (specified above) that should only have a\nsingle value. This will be validated and unwrapped into `nil` | `single_value` on success.\n\n`identifier` expresses that a given entity is unique by that field, validated by the DSL.","ref":"Spark.Dsl.Entity.html"},{"type":"module","title":"Example - Spark.Dsl.Entity","doc":"```elixir\n@my_entity %Spark.Dsl.Entity{\n  name: :my_entity,\n  target: MyStruct,\n  schema: [my_field: [type: :atom, required: false]]\n}\n```\n\nOnce compiled by Spark, entities can be invoked with a keyword list:\n\n```elixir\nmy_entity my_field: :value\n```\n\nOr with a do block:\n\n```elixir\nmy_entity do\n  my_field :value\nend\n```\n\nFor a full example, see `Spark.Dsl.Extension`.","ref":"Spark.Dsl.Entity.html#module-example"},{"type":"type","title":"Spark.Dsl.Entity.args/0","doc":"Specifies positional arguments for an Entity.\n\nAn entity declared like this:\n\n```elixir\n@entity %Spark.Dsl.Entity{\n  name: :entity,\n  target: Entity,\n  schema: [\n    positional: [type: :atom, required: true],\n    other: [type: :atom, required: false],\n  ],\n  args: [:positional]\n}\n```\n\nCan be instantiated like this:\n\n```elixir\nentity :positional_argument do\n  other :other_argument\nend\n```","ref":"Spark.Dsl.Entity.html#t:args/0"},{"type":"type","title":"Spark.Dsl.Entity.auto_set_fields/0","doc":"Set the provided key value pairs in the produced struct. These fields do not need to be included in the Entity's schema.","ref":"Spark.Dsl.Entity.html#t:auto_set_fields/0"},{"type":"type","title":"Spark.Dsl.Entity.deprecations/0","doc":"","ref":"Spark.Dsl.Entity.html#t:deprecations/0"},{"type":"type","title":"Spark.Dsl.Entity.describe/0","doc":"User provided documentation.\n\nDocumentation provided in a `Entity`'s `describe` field will be included by `Spark` in any generated documentation that includes the `Entity`.","ref":"Spark.Dsl.Entity.html#t:describe/0"},{"type":"type","title":"Spark.Dsl.Entity.docs/0","doc":"Internal field. Not set by user.","ref":"Spark.Dsl.Entity.html#t:docs/0"},{"type":"type","title":"Spark.Dsl.Entity.entities/0","doc":"A keyword list of nested entities.","ref":"Spark.Dsl.Entity.html#t:entities/0"},{"type":"type","title":"Spark.Dsl.Entity.examples/0","doc":"","ref":"Spark.Dsl.Entity.html#t:examples/0"},{"type":"type","title":"Spark.Dsl.Entity.hide/0","doc":"","ref":"Spark.Dsl.Entity.html#t:hide/0"},{"type":"type","title":"Spark.Dsl.Entity.id/0","doc":"","ref":"Spark.Dsl.Entity.html#t:id/0"},{"type":"type","title":"Spark.Dsl.Entity.imports/0","doc":"","ref":"Spark.Dsl.Entity.html#t:imports/0"},{"type":"type","title":"Spark.Dsl.Entity.links/0","doc":"","ref":"Spark.Dsl.Entity.html#t:links/0"},{"type":"type","title":"Spark.Dsl.Entity.modules/0","doc":"","ref":"Spark.Dsl.Entity.html#t:modules/0"},{"type":"type","title":"Spark.Dsl.Entity.name/0","doc":"","ref":"Spark.Dsl.Entity.html#t:name/0"},{"type":"type","title":"Spark.Dsl.Entity.no_depend_modules/0","doc":"","ref":"Spark.Dsl.Entity.html#t:no_depend_modules/0"},{"type":"type","title":"Spark.Dsl.Entity.recursive_as/0","doc":"","ref":"Spark.Dsl.Entity.html#t:recursive_as/0"},{"type":"type","title":"Spark.Dsl.Entity.singleton_entity_keys/0","doc":"","ref":"Spark.Dsl.Entity.html#t:singleton_entity_keys/0"},{"type":"type","title":"Spark.Dsl.Entity.snippet/0","doc":"","ref":"Spark.Dsl.Entity.html#t:snippet/0"},{"type":"type","title":"Spark.Dsl.Entity.t/0","doc":"","ref":"Spark.Dsl.Entity.html#t:t/0"},{"type":"type","title":"Spark.Dsl.Entity.target/0","doc":"Defines the struct that will be built from this entity definition.\n\nThe struct will need to have fields for all [`entities`](#t:entities/0), `t:schema/0` fields, and `t:auto_set_fields/0`.","ref":"Spark.Dsl.Entity.html#t:target/0"},{"type":"type","title":"Spark.Dsl.Entity.transform/0","doc":"Specifies a function that will run on the target struct after building.\n\n```elixir\n@my_entity %Spark.Dsl.Entity{\n  name: :my_entity,\n  target: MyEntity,\n  schema: [\n    my_field: [type: :list, required: true]\n  ],\n  transform: {MyModule, :max_three_items, []}\n}\n\ndef max_three_items(my_entity) do\n  if length(my_entity.my_field) > 3 do\n    {:error, \"Can't have more than three items\"}\n  else\n    {:ok, my_entity}\n  end\nend\n```","ref":"Spark.Dsl.Entity.html#t:transform/0"},{"type":"behaviour","title":"Spark.Dsl.Extension","doc":"An extension to the Spark DSL.\n\nThis allows configuring custom DSL components, whose configurations\ncan then be read back. This guide is still a work in progress, but should\nserve as a decent example of what is possible. Open issues on Github if you\nhave any issues/something is unclear.\n\nThe example at the bottom shows how you might build a (not very contextually\nrelevant) DSL extension that would be used like so:\n\n    defmodule MyApp.Vehicle do\n      use Spark.Dsl\n    end\n\n    defmodule MyApp.MyResource do\n      use MyApp.Vehicle,\n        extensions: [MyApp.CarExtension]\n\n      cars do\n        car :ford, :focus, trim: :sedan\n        car :toyota, :corolla\n      end\n    end\n\nThe extension:\n\n    defmodule MyApp.CarExtension do\n      @car_schema [\n        make: [\n          type: :atom,\n          required: true,\n          doc: \"The make of the car\"\n        ],\n        model: [\n          type: :atom,\n          required: true,\n          doc: \"The model of the car\"\n        ],\n        type: [\n          type: :atom,\n          required: true,\n          doc: \"The type of the car\",\n          default: :sedan\n        ]\n      ]\n\n      @car %Spark.Dsl.Entity{\n        name: :car,\n        describe: \"Adds a car\",\n        examples: [\n          \"car :ford, :focus\"\n        ],\n        target: MyApp.Car,\n        args: [:make, :model],\n        schema: @car_schema\n      }\n\n      @cars %Spark.Dsl.Section{\n        name: :cars, # The DSL constructor will be `cars`\n        describe: \"\"\"\n        Configure what cars are available.\n\n        More, deeper explanation. Always have a short one liner explanation,\n        an empty line, and then a longer explanation.\n        \"\"\",\n        entities: [\n          @car # See `Spark.Dsl.Entity` docs\n        ],\n        schema: [\n          default_manufacturer: [\n            type: :atom,\n            doc: \"The default manufacturer\"\n          ]\n        ]\n      }\n\n      use Spark.Dsl.Extension, sections: [@cars]\n    end\n\n\nOften, we will need to do complex validation/validate based on the configuration\nof other resources. Due to the nature of building compile time DSLs, there are\nmany restrictions around that process. To support these complex use cases, extensions\ncan include `transformers` which can validate/transform the DSL state after all basic\nsections/entities have been created. See `Spark.Dsl.Transformer` for more information.\nTransformers are provided as an option to `use`, like so:\n\n    use Spark.Dsl.Extension, sections: [@cars], transformers: [\n      MyApp.Transformers.ValidateNoOverlappingMakesAndModels\n    ]\n\nBy default, the generated modules will have names like `__MODULE__.SectionName.EntityName`, and that could\npotentially conflict with modules you are defining, so you can specify the `module_prefix` option, which would allow\nyou to prefix the modules with something like `__MODULE__.Dsl`, so that the module path generated might be something like\n`__MODULE__.Dsl.SectionName.EntityName`, and you could then have the entity struct be `__MODULE__.SectionName.EntityName`\nwithout conflicts.\n\nTo expose the configuration of your DSL, define functions that use the\nhelpers like `get_entities/2` and `get_opt/3`. For example:\n\n    defmodule MyApp.Cars do\n      def cars(resource) do\n        Spark.Dsl.Extension.get_entities(resource, [:cars])\n      end\n    end\n\n    MyApp.Cars.cars(MyResource)\n    # [%MyApp.Car{...}, %MyApp.Car{...}]\n\nSee the documentation for `Spark.Dsl.Section` and `Spark.Dsl.Entity` for more information","ref":"Spark.Dsl.Extension.html"},{"type":"callback","title":"Spark.Dsl.Extension.add_extensions/0","doc":"","ref":"Spark.Dsl.Extension.html#c:add_extensions/0"},{"type":"function","title":"Spark.Dsl.Extension.doc/2","doc":"","ref":"Spark.Dsl.Extension.html#doc/2"},{"type":"function","title":"Spark.Dsl.Extension.doc_index/2","doc":"","ref":"Spark.Dsl.Extension.html#doc_index/2"},{"type":"function","title":"Spark.Dsl.Extension.expand_alias/2","doc":"","ref":"Spark.Dsl.Extension.html#expand_alias/2"},{"type":"function","title":"Spark.Dsl.Extension.expand_alias_no_require/2","doc":"","ref":"Spark.Dsl.Extension.html#expand_alias_no_require/2"},{"type":"function","title":"Spark.Dsl.Extension.expand_literals/3","doc":"","ref":"Spark.Dsl.Extension.html#expand_literals/3"},{"type":"callback","title":"Spark.Dsl.Extension.explain/1","doc":"","ref":"Spark.Dsl.Extension.html#c:explain/1"},{"type":"function","title":"Spark.Dsl.Extension.fetch_opt/4","doc":"","ref":"Spark.Dsl.Extension.html#fetch_opt/4"},{"type":"function","title":"Spark.Dsl.Extension.fetch_persisted/2","doc":"Fetch a value that was persisted while transforming or compiling the resource, e.g `:primary_key`","ref":"Spark.Dsl.Extension.html#fetch_persisted/2"},{"type":"function","title":"Spark.Dsl.Extension.get_entities/2","doc":"Get the entities configured for a given section","ref":"Spark.Dsl.Extension.html#get_entities/2"},{"type":"function","title":"Spark.Dsl.Extension.get_entity_dsl_patches/2","doc":"","ref":"Spark.Dsl.Extension.html#get_entity_dsl_patches/2"},{"type":"function","title":"Spark.Dsl.Extension.get_opt/5","doc":"Get an option value for a section at a given path.\n\nChecks to see if it has been overridden via configuration.","ref":"Spark.Dsl.Extension.html#get_opt/5"},{"type":"function","title":"Spark.Dsl.Extension.get_opt_config/3","doc":"","ref":"Spark.Dsl.Extension.html#get_opt_config/3"},{"type":"function","title":"Spark.Dsl.Extension.get_persisted/3","doc":"Get a value that was persisted while transforming or compiling the resource, e.g `:primary_key`","ref":"Spark.Dsl.Extension.html#get_persisted/3"},{"type":"function","title":"Spark.Dsl.Extension.get_recursive_entities_for_path/2","doc":"","ref":"Spark.Dsl.Extension.html#get_recursive_entities_for_path/2"},{"type":"function","title":"Spark.Dsl.Extension.module_concat/1","doc":"","ref":"Spark.Dsl.Extension.html#module_concat/1"},{"type":"callback","title":"Spark.Dsl.Extension.module_imports/0","doc":"","ref":"Spark.Dsl.Extension.html#c:module_imports/0"},{"type":"function","title":"Spark.Dsl.Extension.monotonic_number/1","doc":"","ref":"Spark.Dsl.Extension.html#monotonic_number/1"},{"type":"callback","title":"Spark.Dsl.Extension.persisters/0","doc":"","ref":"Spark.Dsl.Extension.html#c:persisters/0"},{"type":"function","title":"Spark.Dsl.Extension.run_transformers/4","doc":"","ref":"Spark.Dsl.Extension.html#run_transformers/4"},{"type":"callback","title":"Spark.Dsl.Extension.sections/0","doc":"","ref":"Spark.Dsl.Extension.html#c:sections/0"},{"type":"function","title":"Spark.Dsl.Extension.shuffle_opts_to_end/5","doc":"","ref":"Spark.Dsl.Extension.html#shuffle_opts_to_end/5"},{"type":"function","title":"Spark.Dsl.Extension.spark_function_info/1","doc":"","ref":"Spark.Dsl.Extension.html#spark_function_info/1"},{"type":"callback","title":"Spark.Dsl.Extension.transformers/0","doc":"","ref":"Spark.Dsl.Extension.html#c:transformers/0"},{"type":"callback","title":"Spark.Dsl.Extension.verifiers/0","doc":"","ref":"Spark.Dsl.Extension.html#c:verifiers/0"},{"type":"type","title":"Spark.Dsl.Extension.t/0","doc":"","ref":"Spark.Dsl.Extension.html#t:t/0"},{"type":"module","title":"Spark.Dsl.Fragment","doc":"Allows splitting up a DSL into multiple modules, potentially organizing large DSLs\n\nUse the `of` option to expression what your fragment is a fragment of. You can add\nextensions as you would normally to that resource, and they will be added to the\nparent resource.\n\n    defmodule MyApp.Resource.Graphql do\n      use Spark.Dsl.Fragment, of: Ash.Resource, extensions: AshGraphql.Resource\n\n      graphql do\n        ...\n      end\n    end\n\nThen add the fragment to the parent resource.\n\n    defmodule MyApp.Resource do\n      use Ash.Resource, fragments: [MyApp.Resource.Graphql], ...\n    end","ref":"Spark.Dsl.Fragment.html"},{"type":"module","title":"Spark.Dsl.Patch.AddEntity","doc":"Supply this when defining an extension to add entity builders to another extension's section.\n\nFor example\n\n```elixir\n@entity %Spark.Dsl.Entity{\n  ...\n}\n\n@dsl_patch %Spark.Dsl.Patch.AddEntity{section_path: [:foo, :bar], entity: @entity}\n\nuse Spark.Dsl.Extension, dsl_patches: [@dsl_patch]\n```","ref":"Spark.Dsl.Patch.AddEntity.html"},{"type":"type","title":"Spark.Dsl.Patch.AddEntity.t/0","doc":"","ref":"Spark.Dsl.Patch.AddEntity.html#t:t/0"},{"type":"module","title":"Spark.Dsl.Section","doc":"Declares a DSL section.\n\nA dsl section allows you to organize related configurations. All extensions\nconfigure sections, they cannot add DSL builders to the top level. This\nkeeps things organized, and concerns separated.\n\nA section may have nested sections, which will be configured the same as other sections.\nGetting the options/entities of a section is done by providing a path, so you would\nuse the nested path to retrieve that configuration. See `Spark.Dsl.Extension.get_entities/2`\nand `Spark.Dsl.Extension.get_opt/4`.\n\nA section may have entities, which are constructors that produce instances of structs.\nFor more on entities, see `Spark.Dsl.Entity`.\n\nA section may also have a `schema`, which you can learn more about in `Spark.Options`. Spark will produce\nbuilders for those options, so that they may be configured. They are retrieved with\n`Spark.Dsl.Extension.get_opt/4`.\n\nTo create a section that is available at the top level (i.e not  nested inside of its own name), use\n`top_level?: true`. Remember, however, that this has no effect on sections nested inside of other sections.\n\nFor a full example, see `Spark.Dsl.Extension`.","ref":"Spark.Dsl.Section.html"},{"type":"type","title":"Spark.Dsl.Section.auto_set_fields/0","doc":"","ref":"Spark.Dsl.Section.html#t:auto_set_fields/0"},{"type":"type","title":"Spark.Dsl.Section.describe/0","doc":"User provided documentation.\n\nDocumentation provided in a `Section`'s `describe` field will be included by `Spark` in any generated documentation that includes the `Section`.","ref":"Spark.Dsl.Section.html#t:describe/0"},{"type":"type","title":"Spark.Dsl.Section.docs/0","doc":"Internal field. Not set by user.","ref":"Spark.Dsl.Section.html#t:docs/0"},{"type":"type","title":"Spark.Dsl.Section.entities/0","doc":"","ref":"Spark.Dsl.Section.html#t:entities/0"},{"type":"type","title":"Spark.Dsl.Section.examples/0","doc":"","ref":"Spark.Dsl.Section.html#t:examples/0"},{"type":"type","title":"Spark.Dsl.Section.imports/0","doc":"","ref":"Spark.Dsl.Section.html#t:imports/0"},{"type":"type","title":"Spark.Dsl.Section.links/0","doc":"","ref":"Spark.Dsl.Section.html#t:links/0"},{"type":"type","title":"Spark.Dsl.Section.modules/0","doc":"","ref":"Spark.Dsl.Section.html#t:modules/0"},{"type":"type","title":"Spark.Dsl.Section.name/0","doc":"","ref":"Spark.Dsl.Section.html#t:name/0"},{"type":"type","title":"Spark.Dsl.Section.no_depend_modules/0","doc":"","ref":"Spark.Dsl.Section.html#t:no_depend_modules/0"},{"type":"type","title":"Spark.Dsl.Section.patchable?/0","doc":"","ref":"Spark.Dsl.Section.html#t:patchable?/0"},{"type":"type","title":"Spark.Dsl.Section.sections/0","doc":"","ref":"Spark.Dsl.Section.html#t:sections/0"},{"type":"type","title":"Spark.Dsl.Section.snippet/0","doc":"","ref":"Spark.Dsl.Section.html#t:snippet/0"},{"type":"type","title":"Spark.Dsl.Section.t/0","doc":"","ref":"Spark.Dsl.Section.html#t:t/0"},{"type":"type","title":"Spark.Dsl.Section.top_level?/0","doc":"Determines whether a section can be declared directly in a module.\n\nWhen `top_level?: true`, that Section's DSL can be declared outside of a `do` block in a module.","ref":"Spark.Dsl.Section.html#t:top_level?/0"},{"type":"type","title":"Example - Spark.Dsl.Section.top_level?/0","doc":"A `Section` declared with `top_level?: true`:\n\n```elixir\n@my_section %Spark.Dsl.Section{\n  top_level?: true,\n  name: :my_section,\n  schema: [my_field: [type: :atom, required: true]]\n}\n```\n\nCan be declared like this:\n\n```elixir\ndefmodule MyDslModule do\n  my_field :value\nend\n```\n\nWith `top_level?: false`, the DSL section would need to be declared explicitly/:\n\n```elixir\ndefmodule MyDslModule do\n  my_section do\n    my_field :value\n  end\nend\n```","ref":"Spark.Dsl.Section.html#t:top_level?/0-example"},{"type":"behaviour","title":"Spark.Dsl.Transformer","doc":"A transformer manipulates and/or validates the entire DSL state of a resource.\n\nIt's `transform/1` takes a `map`, which is just the values/configurations at each point\nof the DSL. Don't manipulate it directly, if possible, instead use functions like\n`get_entities/3` and `replace_entity/4` to manipulate it.\n\nUse the `after?/1` and `before?/1` callbacks to ensure that your transformer\nruns either before or after some other transformer.\n\nReturn `true` in `after_compile/0` to have the transformer run in an `after_compile` hook,\nbut keep in mind that no modifications to the dsl structure will be retained, so there is no\nreal point in modifying the dsl that you return.","ref":"Spark.Dsl.Transformer.html"},{"type":"function","title":"Spark.Dsl.Transformer.add_entity/4","doc":"","ref":"Spark.Dsl.Transformer.html#add_entity/4"},{"type":"callback","title":"Spark.Dsl.Transformer.after?/1","doc":"","ref":"Spark.Dsl.Transformer.html#c:after?/1"},{"type":"callback","title":"Spark.Dsl.Transformer.after_compile?/0","doc":"","ref":"Spark.Dsl.Transformer.html#c:after_compile?/0"},{"type":"function","title":"Spark.Dsl.Transformer.async_compile/2","doc":"Runs the function in an async compiler.\n\nUse this for compiling new modules and having them compiled\nefficiently asynchronously.","ref":"Spark.Dsl.Transformer.html#async_compile/2"},{"type":"callback","title":"Spark.Dsl.Transformer.before?/1","doc":"","ref":"Spark.Dsl.Transformer.html#c:before?/1"},{"type":"function","title":"Spark.Dsl.Transformer.build_entity/4","doc":"","ref":"Spark.Dsl.Transformer.html#build_entity/4"},{"type":"function","title":"Spark.Dsl.Transformer.build_entity!/4","doc":"","ref":"Spark.Dsl.Transformer.html#build_entity!/4"},{"type":"function","title":"Spark.Dsl.Transformer.eval/3","doc":"Add a quoted expression to be evaluated in the DSL module's context.\n\nUse this *extremely sparingly*. It should almost never be necessary, unless building certain\nextensions that *require* the module in question to define a given function.\n\nWhat you likely want is either one of the DSL introspection functions, like `Spark.Dsl.Extension.get_entities/2`\nor `Spark.Dsl.Extension.get_opt/5)`. If you simply want to store a custom value that can be retrieved easily, or\ncache some precomputed information onto the resource, use `persist/3`.\n\nProvide the dsl state, bindings that should be unquote-able, and the quoted block\nto evaluate in the module. For example, if we wanted to support a `resource.primary_key()` function\nthat would return the primary key (this is unnecessary, just an example), we might do this:\n\n```elixir\nfields = the_primary_key_fields\n\ndsl_state =\n  Transformer.eval(\n    dsl_state,\n    [fields: fields],\n    quote do\n      def primary_key() do\n        unquote(fields)\n      end\n    end\n  )\n```","ref":"Spark.Dsl.Transformer.html#eval/3"},{"type":"function","title":"Spark.Dsl.Transformer.fetch_option/3","doc":"","ref":"Spark.Dsl.Transformer.html#fetch_option/3"},{"type":"function","title":"Spark.Dsl.Transformer.fetch_persisted/2","doc":"","ref":"Spark.Dsl.Transformer.html#fetch_persisted/2"},{"type":"function","title":"Spark.Dsl.Transformer.get_entities/2","doc":"","ref":"Spark.Dsl.Transformer.html#get_entities/2"},{"type":"function","title":"Spark.Dsl.Transformer.get_option/4","doc":"","ref":"Spark.Dsl.Transformer.html#get_option/4"},{"type":"function","title":"Spark.Dsl.Transformer.get_persisted/3","doc":"","ref":"Spark.Dsl.Transformer.html#get_persisted/3"},{"type":"function","title":"Spark.Dsl.Transformer.persist/3","doc":"Saves a value into the dsl config with the given key.\n\nThis can be used to precompute some information and cache it onto the resource,\nor simply store a computed value. It can later be retrieved with `Spark.Dsl.Extension.get_persisted/3`.","ref":"Spark.Dsl.Transformer.html#persist/3"},{"type":"function","title":"Spark.Dsl.Transformer.remove_entity/3","doc":"","ref":"Spark.Dsl.Transformer.html#remove_entity/3"},{"type":"function","title":"Spark.Dsl.Transformer.replace_entity/4","doc":"","ref":"Spark.Dsl.Transformer.html#replace_entity/4"},{"type":"function","title":"Spark.Dsl.Transformer.set_option/4","doc":"","ref":"Spark.Dsl.Transformer.html#set_option/4"},{"type":"function","title":"Spark.Dsl.Transformer.sort/1","doc":"","ref":"Spark.Dsl.Transformer.html#sort/1"},{"type":"callback","title":"Spark.Dsl.Transformer.transform/1","doc":"","ref":"Spark.Dsl.Transformer.html#c:transform/1"},{"type":"behaviour","title":"Spark.Dsl.Verifier","doc":"A verifier gets the dsl state and can return `:ok` or `:error`.\n\nIn a verifier, you can reference and depend on other modules without causing compile time dependencies.","ref":"Spark.Dsl.Verifier.html"},{"type":"function","title":"Spark.Dsl.Verifier.fetch_option/3","doc":"","ref":"Spark.Dsl.Verifier.html#fetch_option/3"},{"type":"function","title":"Spark.Dsl.Verifier.get_entities/2","doc":"","ref":"Spark.Dsl.Verifier.html#get_entities/2"},{"type":"function","title":"Spark.Dsl.Verifier.get_option/3","doc":"","ref":"Spark.Dsl.Verifier.html#get_option/3"},{"type":"function","title":"Spark.Dsl.Verifier.get_option/4","doc":"","ref":"Spark.Dsl.Verifier.html#get_option/4"},{"type":"function","title":"Spark.Dsl.Verifier.get_persisted/2","doc":"","ref":"Spark.Dsl.Verifier.html#get_persisted/2"},{"type":"function","title":"Spark.Dsl.Verifier.get_persisted/3","doc":"","ref":"Spark.Dsl.Verifier.html#get_persisted/3"},{"type":"callback","title":"Spark.Dsl.Verifier.verify/1","doc":"","ref":"Spark.Dsl.Verifier.html#c:verify/1"},{"type":"module","title":"Spark.Dsl.Verifiers.VerifyEntityUniqueness","doc":"Verifies that each entity that has an identifier is unique at each path.","ref":"Spark.Dsl.Verifiers.VerifyEntityUniqueness.html"},{"type":"function","title":"Spark.Dsl.Verifiers.VerifyEntityUniqueness.verify/1","doc":"","ref":"Spark.Dsl.Verifiers.VerifyEntityUniqueness.html#verify/1"},{"type":"module","title":"Spark.Options","doc":"Provides a standard API to handle keyword-list-based options.\n\nThis module began its life as a vendored form of `NimbleOptions`.\nWe had various features to add to it, and the spirit of nimble\noptions is to be as lightweight as possible. With that in mind,\nwe were advised to vendor `NimbleOptions`. We would like to\nthank the authors of `NimbleOptions` for their excellent work,\nand their blessing to transplant their work into Spark.\n\n`Spark.Options` allows developers to create schemas using a\npre-defined set of options and types. The main benefits are:\n\n  * A single unified way to define simple static options\n  * Config validation against schemas\n  * Automatic doc generation\n\n\nSpark also uses this to power entity and section schemas.","ref":"Spark.Options.html"},{"type":"module","title":"Schema Options - Spark.Options","doc":"These are the options supported in a *schema*. They are what\ndefines the validation for the items in the given schema.\n\n* `:type` - The type of the option item. The default value is `:any`.\n\n* `:required` (`t:boolean/0`) - Defines if the option item is required. The default value is `false`.\n\n* `:default` (`t:term/0`) - The default value for the option item if that option is not specified. This value\n  is *validated* according to the given `:type`. This means that you cannot\n  have, for example, `type: :integer` and use `default: \"a string\"`.\n\n* `:keys` (`t:keyword/0`) - Available for types `:keyword_list`, `:non_empty_keyword_list`, and `:map`,\n  it defines which set of keys are accepted for the option item. The value of the\n  `:keys` option is a schema itself. For example: `keys: [foo: [type: :atom]]`.\n  Use `:*` as the key to allow multiple arbitrary keys and specify their schema:\n  `keys: [*: [type: :integer]]`.\n\n* `:deprecated` (`t:String.t/0`) - Defines a message to indicate that the option item is deprecated. The message will be displayed as a warning when passing the item.\n\n* `:private?` (`t:boolean/0`) - Defines an option as private, used with `Spark.Options.Validator` The default value is `false`.\n\n* `:hide` (one or a list of `t:atom/0`) - A list of keys that should be hidden when generating documentation\n\n* `:as` (`t:atom/0`) - A name to remap the option to when used in DSLs. Not supported in regular option parsing\n\n* `:snippet` (`t:String.t/0`) - A snippet to use when autocompleting DSLs. Not supported in regular option parsing\n\n* `:links` (`t:term/0`) - A keyword list of links to include in DSL documentation for the option item.\n\n* `:doc` (`t:String.t/0` or `false`) - The documentation for the option item.\n\n* `:subsection` (`t:String.t/0`) - The title of separate subsection of the options' documentation\n\n* `:type_doc` (`t:String.t/0` or `false`) - The type doc to use *in the documentation* for the option item. If `false`,\n  no type documentation is added to the item. If it's a string, it can be\n  anything. For example, you can use `\"a list of PIDs\"`, or you can use\n  a typespec reference that ExDoc can link to the type definition, such as\n  `` \"`t:binary/0`\" ``. You can use Markdown in this documentation. If the\n  `:type_doc` option is not present, Spark.Options tries to produce a type\n  documentation automatically if it can do it unambiguously. For example,\n  if `type: :integer`, Spark.Options will use `t:integer/0` as the\n  auto-generated type doc.\n\n* `:type_spec` (`t:Macro.t/0`) - The quoted spec to use *in the typespec* for the option item. You should use this\n  when the auto-generated spec is not specific enough. For example, if you are performing\n  custom validation on an option (with the `{:custom, ...}` type), then the\n  generated type spec for that option will always be `t:term/0`, but you can use\n  this option to customize that. The value for this option **must** be a quoted Elixir\n  term. For example, if you have an `:exception` option that is validated with a\n  `{:custom, ...}` type (based on `is_exception/1`), you can override the type\n  spec for that option to be `quote(do: Exception.t())`. *Available since v1.1.0*.","ref":"Spark.Options.html#module-schema-options"},{"type":"module","title":"Types - Spark.Options","doc":"* `:any` - Any type.\n\n  * `:keyword_list` - A keyword list.\n\n  * `:non_empty_keyword_list` - A non-empty keyword list.\n\n  * `:map` - A map consisting of `:atom` keys. Shorthand for `{:map, :atom, :any}`.\n    Keys can be specified using the `keys` option.\n\n  * `{:map, key_type, value_type}` - A map consisting of `key_type` keys and\n    `value_type` values.\n\n  * `:atom` - An atom.\n\n  * `:string` - A string.\n\n  * `:boolean` - A boolean.\n\n  * `:integer` - An integer.\n\n  * `:non_neg_integer` - A non-negative integer.\n\n  * `:pos_integer` - A positive integer.\n\n  * `:float` - A float.\n\n  * `:timeout` - A non-negative integer or the atom `:infinity`.\n\n  * `:pid` - A PID (process identifier).\n\n  * `:reference` - A reference (see `t:reference/0`).\n\n  * `nil` - The value `nil` itself. Available since v1.0.0.\n\n  * `:mfa` - A named function in the format `{module, function, arity}` where\n    `arity` is a list of arguments. For example, `{MyModule, :my_fun, [arg1, arg2]}`.\n\n  * `:mod_arg` - A module along with arguments, such as `{MyModule, arguments}`.\n    Usually used for process initialization using `start_link` and similar. The\n    second element of the tuple can be any term.\n\n  * `:fun` - Any function.\n\n  * `{:fun, arity}` - Any function with the specified arity.\n\n  * `{:fun, args_types}` - A function with the specified arguments.\n\n  * `{:fun, args_types, return_type}` - A function with the specified arguments and return type.\n\n  * `{:in, choices}` or `{:one_of, choices}` - A value that is a member of one of the `choices`. `choices`\n    should be a list of terms or a `Range`. The value is an element in said\n    list of terms, that is, `value in choices` is `true`.\n\n  * `{:struct, struct_name}` - An instance of the struct type given.\n\n  * `:struct` - An instance of any struct\n\n  * `{:tagged_tuple, tag, inner_type}` - maps to `{tag, type}`\n\n  * `{:spark_behaviour, behaviour}` - expects a module that implements the given behaviour, and can be specified with options, i.e `mod` or `{mod, [opt: :val]}`\n\n  * `{:spark_behaviour, behaviour, builtin_module}` - Same as the above, but also accepts a `builtin_module`. The builtin_module is used to provide additional options for the elixir_sense plugin.\n\n  * `{:spark_function_behaviour, behaviour, {function_mod, arity}}` - expects a module that implements the given behaviour, and can be specified with options, i.e `mod` or `{mod, [opt: :val]}`, that also has a special module that supports being provided an anonymous function or MFA as the `:fun` option.\n\n  * `{:spark_function_behaviour, behaviour, builtin_module, {function_mod, arity}}` - Same as the above, but also accepts a `builtin_module`. The builtin_module is used to provide additional options for the elixir_sense plugin.\n\n  * `{:behaviour, behaviour}` - expects a module that implements a given behaviour.\n\n  * `{:protocol, protocol}` - expects a value for which the protocol is implemented.\n\n  * `{:spark, dsl_module}` - expects a module that is a `Spark.Dsl`\n\n  * `{:mfa_or_fun, arity}` - expects a function or MFA of a corresponding arity.\n\n  * `{:spark_type, module, builtin_function}` - a behaviour that defines `builtin_function/0` that returns a list of atoms that map to built in variations of that thing.\n\n  * `{:spark_type, module, builtin_function, templates}` - same as the above, but includes additional templates for elixir_sense autocomplete\n\n  * `:literal` -> any literal value. Maps to `:any`, but is used for documentation.\n\n  * `{:literal, value}` -> exactly the value specified.\n\n  * `:quoted` -> retains the quoted value of the code provided to the option\n\n  * `{:wrap_list, type}` -> Allows a single value or a list of values.\n\n\n  * `{:custom, mod, fun, args}` - A custom type. The related value must be validated\n    by `mod.fun(values, ...args)`. The function should return `{:ok, value}` or\n    `{:error, message}`.\n\n  * `{:or, subtypes}` - A value that matches one of the given `subtypes`. The value is\n    matched against the subtypes in the order specified in the list of `subtypes`. If\n    one of the subtypes matches and **updates** (casts) the given value, the updated\n    value is used. For example: `{:or, [:string, :boolean, {:fun, 2}]}`. If one of the\n    subtypes is a keyword list or map, you won't be able to pass `:keys` directly. For this reason,\n    `:keyword_list`, `:non_empty_keyword_list`, and `:map` are special cased and can\n    be used as subtypes with `{:keyword_list, keys}`, `{:non_empty_keyword_list, keys}` or `{:map, keys}`.\n    For example, a type such as `{:or, [:boolean, keyword_list: [enabled: [type: :boolean]]]}`\n    would match either a boolean or a keyword list with the `:enabled` boolean option in it.\n\n  * `{:list, subtype}` - A list where all elements match `subtype`. `subtype` can be any\n    of the accepted types listed here. Empty lists are allowed. The resulting validated list\n    contains the validated (and possibly updated) elements, each as returned after validation\n    through `subtype`. For example, if `subtype` is a custom validator function that returns\n    an updated value, then that updated value is used in the resulting list. Validation\n    fails at the *first* element that is invalid according to `subtype`. If `subtype` is\n    a keyword list or map, you won't be able to pass `:keys` directly. For this reason,\n    `:keyword_list`, `:non_empty_keyword_list`, and `:map` are special cased and can\n    be used as the subtype by using `{:keyword_list, keys}`, `{:non_empty_keyword_list, keys}`\n    or `{:keyword_list, keys}`. For example, a type such as\n    `{:list, {:keyword_list, enabled: [type: :boolean]}}` would a *list of keyword lists*,\n    where each keyword list in the list could have the `:enabled` boolean option in it.\n\n  * `{:tuple, list_of_subtypes}` - A tuple as described by `tuple_of_subtypes`.\n    `list_of_subtypes` must be a list with the same length as the expected tuple.\n    Each of the list's elements must be a subtype that should match the given element in that\n    same position. For example, to describe 3-element tuples with an atom, a string, and\n    a list of integers you would use the type `{:tuple, [:atom, :string, {:list, :integer}]}`.\n    *Available since v0.4.1*.","ref":"Spark.Options.html#module-types"},{"type":"module","title":"Example - Spark.Options","doc":"iex> schema = [\n    ...>   producer: [\n    ...>     type: :non_empty_keyword_list,\n    ...>     required: true,\n    ...>     keys: [\n    ...>       module: [required: true, type: :mod_arg],\n    ...>       concurrency: [\n    ...>         type: :pos_integer,\n    ...>       ]\n    ...>     ]\n    ...>   ]\n    ...> ]\n    ...>\n    ...> config = [\n    ...>   producer: [\n    ...>     concurrency: 1,\n    ...>   ]\n    ...> ]\n    ...>\n    ...> {:error, %Spark.Options.ValidationError{} = error} = Spark.Options.validate(config, schema)\n    ...> Exception.message(error)\n    \"required :module option not found, received options: [:concurrency] (in options [:producer])\"","ref":"Spark.Options.html#module-example"},{"type":"module","title":"Nested Option Items - Spark.Options","doc":"`Spark.Options` allows option items to be nested so you can recursively validate\nany item down the options tree.","ref":"Spark.Options.html#module-nested-option-items"},{"type":"module","title":"Example - Spark.Options","doc":"iex> schema = [\n    ...>   producer: [\n    ...>     required: true,\n    ...>     type: :non_empty_keyword_list,\n    ...>     keys: [\n    ...>       rate_limiting: [\n    ...>         type: :non_empty_keyword_list,\n    ...>         keys: [\n    ...>           interval: [required: true, type: :pos_integer]\n    ...>         ]\n    ...>       ]\n    ...>     ]\n    ...>   ]\n    ...> ]\n    ...>\n    ...> config = [\n    ...>   producer: [\n    ...>     rate_limiting: [\n    ...>       interval: :oops!\n    ...>     ]\n    ...>   ]\n    ...> ]\n    ...>\n    ...> {:error, %Spark.Options.ValidationError{} = error} = Spark.Options.validate(config, schema)\n    ...> Exception.message(error)\n    \"invalid value for :interval option: expected positive integer, got: :oops! (in options [:producer, :rate_limiting])\"","ref":"Spark.Options.html#module-example"},{"type":"module","title":"Validating Schemas - Spark.Options","doc":"Each time `validate/2` is called, the given schema itself will be validated before validating\nthe options.\n\nIn most applications the schema will never change but validating options will be done\nrepeatedly.\n\nTo avoid the extra cost of validating the schema, it is possible to validate the schema once,\nand then use that valid schema directly. This is done by using the `new!/1` function first, and\nthen passing the returned schema to `validate/2`.\n\n> #### Create the Schema at Compile Time {: .tip}\n>\n> If your option schema doesn't include any runtime-only terms in it (such as anonymous\n> functions), you can call `new!/1` to validate the schema and returned a *compiled* schema\n> **at compile time**. This is an efficient way to avoid doing any unnecessary work at\n> runtime. See the example below for more information.","ref":"Spark.Options.html#module-validating-schemas"},{"type":"module","title":"Example - Spark.Options","doc":"iex> raw_schema = [\n    ...>   hostname: [\n    ...>     required: true,\n    ...>     type: :string\n    ...>   ]\n    ...> ]\n    ...>\n    ...> schema = Spark.Options.new!(raw_schema)\n    ...> Spark.Options.validate([hostname: \"elixir-lang.org\"], schema)\n    {:ok, hostname: \"elixir-lang.org\"}\n\nCalling `new!/1` from a function that receives options will still validate the schema each time\nthat function is called. Declaring the schema as a module attribute is supported:\n\n    @options_schema Spark.Options.new!([...])\n\nThis schema will be validated at compile time. Calling `docs/1` on that schema is also\nsupported.","ref":"Spark.Options.html#module-example"},{"type":"function","title":"Spark.Options.docs/2","doc":"Returns documentation for the given schema.\n\nYou can use this to inject documentation in your docstrings. For example,\nsay you have your schema in a module attribute:\n\n    @options_schema [...]\n\nWith this, you can use `docs/1` to inject documentation:\n\n    @doc \"Supported options:\\n#{Spark.Options.docs(@options_schema)}\"","ref":"Spark.Options.html#docs/2"},{"type":"function","title":"Options - Spark.Options.docs/2","doc":"* `:nest_level` - an integer deciding the \"nest level\" of the generated\n    docs. This is useful when, for example, you use `docs/2` inside the `:doc`\n    option of another schema. For example, if you have the following nested schema:\n\n        nested_schema = [\n          allowed_messages: [type: :pos_integer, doc: \"Allowed messages.\"],\n          interval: [type: :pos_integer, doc: \"Interval.\"]\n        ]\n\n    then you can document it inside another schema with its nesting level increased:\n\n        schema = [\n          producer: [\n            type: {:or, [:string, keyword_list: nested_schema]},\n            doc:\n              \"Either a string or a keyword list with the following keys:\\n\\n\" <>\n                Spark.Options.docs(nested_schema, nest_level: 1)\n          ],\n          other_key: [type: :string]\n        ]","ref":"Spark.Options.html#docs/2-options"},{"type":"function","title":"Spark.Options.merge/3","doc":"Merges two schemas, and sets the `subsection` option on all options on the right side.","ref":"Spark.Options.html#merge/3"},{"type":"function","title":"Spark.Options.new!/1","doc":"Validates the given `schema` and returns a wrapped schema to be used with `validate/2`.\n\nIf the given schema is not valid, raises a `Spark.Options.ValidationError`.","ref":"Spark.Options.html#new!/1"},{"type":"function","title":"Spark.Options.option_typespec/1","doc":"Returns the quoted typespec for any option described by the given schema.\n\nThe returned quoted code represents the **type union** for all possible\nkeys in the schema, alongside their type. Nested keyword lists are\nspec'ed as `t:keyword/0`.","ref":"Spark.Options.html#option_typespec/1"},{"type":"function","title":"Usage - Spark.Options.option_typespec/1","doc":"Because of how typespecs are treated by the Elixir compiler, you have\nto use `unquote/1` on the return value of this function to use it\nin a typespec:\n\n    @type option() :: unquote(Spark.Options.option_typespec(my_schema))\n\nThis function returns the type union for a single option: to give you\nflexibility to combine it and use it in your own typespecs. For example,\nif you only validate part of the options through Spark.Options, you could\nwrite a spec like this:\n\n    @type my_option() ::\n            {:my_opt1, integer()}\n            | {:my_opt2, boolean()}\n            | unquote(Spark.Options.option_typespec(my_schema))\n\nIf you want to spec a whole schema, you could write something like this:\n\n    @type options() :: [unquote(Spark.Options.option_typespec(my_schema))]","ref":"Spark.Options.html#option_typespec/1-usage"},{"type":"function","title":"Example - Spark.Options.option_typespec/1","doc":"schema = [\n      int: [type: :integer],\n      number: [type: {:or, [:integer, :float]}]\n    ]\n\n    @type option() :: unquote(Spark.Options.option_typespec(schema))\n\nThe code above would essentially compile to:\n\n    @type option() :: {:int, integer()} | {:number, integer() | float()}","ref":"Spark.Options.html#option_typespec/1-example"},{"type":"function","title":"Spark.Options.validate/2","doc":"Validates the given `options` with the given `schema`.\n\nSee the module documentation for what a `schema` is.\n\nIf the validation is successful, this function returns `{:ok, validated_options}`\nwhere `validated_options` is a keyword list. If the validation fails, this\nfunction returns `{:error, validation_error}` where `validation_error` is a\n`Spark.Options.ValidationError` struct explaining what's wrong with the options.\nYou can use `raise/1` with that struct or `Exception.message/1` to turn it into a string.","ref":"Spark.Options.html#validate/2"},{"type":"function","title":"Spark.Options.validate!/2","doc":"Validates the given `options` with the given `schema` and raises if they're not valid.\n\nThis function behaves exactly like `validate/2`, but returns the options directly\nif they're valid or raises a `Spark.Options.ValidationError` exception otherwise.","ref":"Spark.Options.html#validate!/2"},{"type":"type","title":"Spark.Options.option_schema/0","doc":"","ref":"Spark.Options.html#t:option_schema/0"},{"type":"type","title":"Spark.Options.schema/0","doc":"A schema.\n\nSee the module documentation for more information.","ref":"Spark.Options.html#t:schema/0"},{"type":"type","title":"Spark.Options.t/0","doc":"The `Spark.Options` struct embedding a validated schema.\n\nSee the [*Validating Schemas* section](#module-validating-schemas) in\nthe module documentation.","ref":"Spark.Options.html#t:t/0"},{"type":"type","title":"Spark.Options.type/0","doc":"","ref":"Spark.Options.html#t:type/0"},{"type":"module","title":"Spark.Options.Helpers","doc":"Helpers for use with spark options","ref":"Spark.Options.Helpers.html"},{"type":"function","title":"Spark.Options.Helpers.append_doc!/3","doc":"","ref":"Spark.Options.Helpers.html#append_doc!/3"},{"type":"function","title":"Spark.Options.Helpers.make_optional!/2","doc":"","ref":"Spark.Options.Helpers.html#make_optional!/2"},{"type":"function","title":"Spark.Options.Helpers.make_required!/2","doc":"","ref":"Spark.Options.Helpers.html#make_required!/2"},{"type":"function","title":"Spark.Options.Helpers.set_default!/3","doc":"","ref":"Spark.Options.Helpers.html#set_default!/3"},{"type":"function","title":"Spark.Options.Helpers.set_type!/3","doc":"","ref":"Spark.Options.Helpers.html#set_type!/3"},{"type":"exception","title":"Spark.Options.ValidationError","doc":"An error that is returned (or raised) when options are invalid.\n\nSince this is an exception, you can either raise it directly with `raise/1`\nor turn it into a message string with `Exception.message/1`.\n\nSee [`%Spark.Options.ValidationError{}`](`__struct__/0`) for documentation on the fields.","ref":"Spark.Options.ValidationError.html"},{"type":"function","title":"Spark.Options.ValidationError.__struct__/0","doc":"The error struct.\n\nOnly the following documented fields are considered public. All other fields are\nconsidered private and should not be referenced:\n\n  * `:key` (`t:atom/0`) - The key that did not successfully validate.\n\n  * `:keys_path` (list of `t:atom/0`) - If the key is nested, this is the path to the key.\n\n  * `:value` (`t:term/0`) - The value that failed to validate. This field is `nil` if there\n    was no value provided.","ref":"Spark.Options.ValidationError.html#__struct__/0"},{"type":"type","title":"Spark.Options.ValidationError.t/0","doc":"","ref":"Spark.Options.ValidationError.html#t:t/0"},{"type":"module","title":"Spark.Options.Validator","doc":"Defines a validator module for an option schema.\n\nValidators create structs with keys for each option in their schema,\nand an optimized `validate`, and `validate!` function on that struct.","ref":"Spark.Options.Validator.html"},{"type":"module","title":"Upgrading from options lists - Spark.Options.Validator","doc":"You can pass the option `define_deprecated_access?: true` to `use Spark.Options.Validator`,\nwhich will make it such that `options[:foo]` will still work, but will emit a deprecation warning.\nThis cane help with smoother upgrades.","ref":"Spark.Options.Validator.html#module-upgrading-from-options-lists"},{"type":"module","title":"Example - Spark.Options.Validator","doc":"Given a module like the following:\n\n```elixir\ndefmodule MyOptions do\n  use Spark.Options.Validator, schema: [\n    foo: [\n      type: :string,\n      required: true\n    ],\n    bar: [\n      type: :string\n    ],\n    baz: [\n      type: :integer,\n      default: 10\n    ]\n  ]\nend\n```\n\nYou can use it like so:\n\n```elixir\n# validate options\n\nMyOptions.validate!(foo: \"foo\")\n# %MyOptions{foo: \"foo\", bar: nil, baz: 10}\n\n# retrieve original schema\nMyOptions.schema()\n# foo: [type: :string, required: true], bar: [type: :string], baz: [type: :integer, default: 10]\n```","ref":"Spark.Options.Validator.html#module-example"},{"type":"module","title":"Spark.OptionsHelpers","doc":"Helpers for working with options lists.","ref":"Spark.OptionsHelpers.html"},{"type":"function","title":"Spark.OptionsHelpers.append_doc!/3","doc":"","ref":"Spark.OptionsHelpers.html#append_doc!/3"},{"type":"function","title":"Spark.OptionsHelpers.docs/1","doc":"Creates markdown documentation for a given schema.","ref":"Spark.OptionsHelpers.html#docs/1"},{"type":"function","title":"Spark.OptionsHelpers.make_optional!/2","doc":"","ref":"Spark.OptionsHelpers.html#make_optional!/2"},{"type":"function","title":"Spark.OptionsHelpers.make_required!/2","doc":"","ref":"Spark.OptionsHelpers.html#make_required!/2"},{"type":"function","title":"Spark.OptionsHelpers.merge_schemas/3","doc":"","ref":"Spark.OptionsHelpers.html#merge_schemas/3"},{"type":"function","title":"Spark.OptionsHelpers.set_default!/3","doc":"","ref":"Spark.OptionsHelpers.html#set_default!/3"},{"type":"function","title":"Spark.OptionsHelpers.set_type!/3","doc":"","ref":"Spark.OptionsHelpers.html#set_type!/3"},{"type":"function","title":"Spark.OptionsHelpers.validate/2","doc":"","ref":"Spark.OptionsHelpers.html#validate/2"},{"type":"function","title":"Spark.OptionsHelpers.validate!/2","doc":"","ref":"Spark.OptionsHelpers.html#validate!/2"},{"type":"exception","title":"Spark.Error.DslError","doc":"Used when a DSL is incorrectly configured.","ref":"Spark.Error.DslError.html"},{"type":"type","title":"Spark.Error.DslError.t/0","doc":"","ref":"Spark.Error.DslError.html#t:t/0"},{"type":"task","title":"mix spark.cheat_sheets","doc":"Creates cheat sheets for each Extension provided. Useful for CI with `--check` flag.","ref":"Mix.Tasks.Spark.CheatSheets.html"},{"type":"function","title":"Mix.Tasks.Spark.CheatSheets.run/1","doc":"","ref":"Mix.Tasks.Spark.CheatSheets.html#run/1"},{"type":"task","title":"mix spark.cheat_sheets_in_search","doc":"Includes generated cheat sheets in the search bar","ref":"Mix.Tasks.Spark.CheatSheetsInSearch.html"},{"type":"function","title":"Mix.Tasks.Spark.CheatSheetsInSearch.run/1","doc":"","ref":"Mix.Tasks.Spark.CheatSheetsInSearch.html#run/1"},{"type":"task","title":"mix spark.formatter","doc":"Manages a variable called `spark_locals_without_parens` in the .formatter.exs from a list of DSL extensions.","ref":"Mix.Tasks.Spark.Formatter.html"},{"type":"function","title":"Mix.Tasks.Spark.Formatter.all_entity_builders_everywhere/3","doc":"","ref":"Mix.Tasks.Spark.Formatter.html#all_entity_builders_everywhere/3"},{"type":"function","title":"Mix.Tasks.Spark.Formatter.run/1","doc":"","ref":"Mix.Tasks.Spark.Formatter.html#run/1"},{"type":"task","title":"mix spark.install","doc":"Installs spark by adding the `Spark.Formatter` plugin, and providing a basic configuration for it in `config.exs`.","ref":"Mix.Tasks.Spark.Install.html"},{"type":"task","title":"mix spark.replace_doc_links","doc":"Replaces any documentation links with text appropriate for hex docs.\n\nThis makes projects support","ref":"Mix.Tasks.Spark.ReplaceDocLinks.html"},{"type":"function","title":"Mix.Tasks.Spark.ReplaceDocLinks.run/1","doc":"","ref":"Mix.Tasks.Spark.ReplaceDocLinks.html#run/1"},{"type":"module","title":"Spark","doc":"Documentation for `Spark`.","ref":"Spark.html"},{"type":"function","title":"Spark.extensions/1","doc":"Returns the extensions a given DSL uses","ref":"Spark.html#extensions/1"},{"type":"function","title":"Spark.implements_behaviour?/2","doc":"Returns true if the module implements the specified behavior","ref":"Spark.html#implements_behaviour?/2"},{"type":"function","title":"Spark.otp_app/1","doc":"Returns the configured otp_app of a given DSL instance","ref":"Spark.html#otp_app/1"},{"type":"function","title":"Spark.sparks/2","doc":"Returns all modules that implement the specified behaviour for a given otp_app.\n\nShould only be called at runtime, not at compile time, as it will have\ninconsistent results at compile time.","ref":"Spark.html#sparks/2"},{"type":"module","title":"Spark.CheatSheet","doc":"Tools to generate cheat sheets for spark DSLs","ref":"Spark.CheatSheet.html"},{"type":"function","title":"Spark.CheatSheet.cheat_sheet/1","doc":"Generate a cheat sheet for a given DSL","ref":"Spark.CheatSheet.html#cheat_sheet/1"},{"type":"function","title":"Spark.CheatSheet.doc/2","doc":"Generate a markdown bullet list documentation for a list of sections","ref":"Spark.CheatSheet.html#doc/2"},{"type":"function","title":"Spark.CheatSheet.doc_index/3","doc":"Generate a table of contents for a list of sections","ref":"Spark.CheatSheet.html#doc_index/3"},{"type":"function","title":"Spark.CheatSheet.section_cheat_sheet/2","doc":"","ref":"Spark.CheatSheet.html#section_cheat_sheet/2"},{"type":"module","title":"Spark.CodeHelpers","doc":"Helpers for meta programming around code and code snippets","ref":"Spark.CodeHelpers.html"},{"type":"function","title":"Spark.CodeHelpers.code_identifier/1","doc":"Given a section of Elixir AST, generate a hash of the code to help with\ngenerating unique names.","ref":"Spark.CodeHelpers.html#code_identifier/1"},{"type":"function","title":"Spark.CodeHelpers.lift_functions/3","doc":"Lift anonymous and captured functions.\n\nActs as an AST transformer to allow these kinds of functions to be added in\nthe AST:\n\nIn the case of captured functions, it ensures they are all captured remote\nfunctions (ie calls with both the module and function name present) - this\noften requires the definition of a new public function on the target module.\n\nIn the case of anonymous functions, it converts them into a new public\nfunction on the module and returns a (remote) function capture much like that\nof above.","ref":"Spark.CodeHelpers.html#lift_functions/3"},{"type":"function","title":"Spark.CodeHelpers.prewalk/2","doc":"Copy of `Macro.prewalk/2` w/ a branch accumulator","ref":"Spark.CodeHelpers.html#prewalk/2"},{"type":"function","title":"Spark.CodeHelpers.prewalk/4","doc":"Copy of `Macro.prewalk/3` w/ a branch accumulator","ref":"Spark.CodeHelpers.html#prewalk/4"},{"type":"function","title":"Spark.CodeHelpers.traverse/5","doc":"A copy of the corresponding `Macro.traverse` function that has a separate accumulator that only goes *down* each branch, only for `pre`","ref":"Spark.CodeHelpers.html#traverse/5"},{"type":"module","title":"Spark.Formatter","doc":"Formats Spark modules.\n\nCurrently, it is very simple, and will only reorder the outermost sections according to some rules.\n\n# Plugin\n\nInclude the plugin into your `.formatter.exs` like so `plugins: [Spark.Formatter]`.\n\nIf no configuration is provided, it will sort all top level DSL sections *alphabetically*.\n\n# Section Order\n\nTo provide a custom section order, add configuration to your app, for example:\n\n```elixir\nconfig :spark, :formatter,\n  remove_parens?: true,\n  \"Ash.Resource\": [\n    section_order: [\n      :resource,\n      :postgres,\n      :attributes,\n      :relationships,\n      :aggregates,\n      :calculations\n    ]\n  ],\n  \"MyApp.Resource\": [\n    # Use this if you use a module that is not the spark DSL itself.\n    # For example, you might have a \"base\" that you use instead that sets some simple defaults.\n\n    # This tells us what the actual thing is so we know what extensions are included automatically.\n    type: Ash.Resource,\n\n    # Tell us what extensions might be added under the hood\n    extensions: [MyApp.ResourceExtension],\n    section_order: [...]\n  ]\n```\n\nAny sections found that aren't in that list will be left in the order that they were in, the sections\nin the list will be sorted \"around\" those sections. E.g the following list: `[:code_interface, :attributes]` can be interpreted as\n\"ensure that code_interface comes before attributes, and don't change the rest\".","ref":"Spark.Formatter.html"},{"type":"function","title":"Spark.Formatter.features/1","doc":"","ref":"Spark.Formatter.html#features/1"},{"type":"function","title":"Spark.Formatter.format/2","doc":"","ref":"Spark.Formatter.html#format/2"},{"type":"module","title":"Spark.Igniter","doc":"Helpers for patching Spark DSLs.","ref":"Spark.Igniter.html"},{"type":"function","title":"Spark.Igniter.add_extension/6","doc":"Adds an extension to a DSL module.","ref":"Spark.Igniter.html#add_extension/6"},{"type":"function","title":"Spark.Igniter.get_option/3","doc":"Gets an option at a given path within a DSL.\nWe will attempt to expand literals using the environment at the path\nbut this is only guaranteed to return the *AST* at that option, *not* necessarily a value.\n\nAdditionally, this only finds options set explicitly in the body of the resource, not by an extension.","ref":"Spark.Igniter.html#get_option/3"},{"type":"function","title":"Spark.Igniter.has_extension/5","doc":"Returns `{igniter, true}` if the module has the extension, or `{igniter, false}` otherwise.","ref":"Spark.Igniter.html#has_extension/5"},{"type":"function","title":"Spark.Igniter.prepend_to_section_order/3","doc":"Prepends a new section or list of sections to the section order in a formatter configuration.","ref":"Spark.Igniter.html#prepend_to_section_order/3"},{"type":"function","title":"Spark.Igniter.remove_extension/6","doc":"Removes an extension from a DSL module.","ref":"Spark.Igniter.html#remove_extension/6"},{"type":"function","title":"Spark.Igniter.set_option/5","doc":"Sets an option at a given path within in a DSL.","ref":"Spark.Igniter.html#set_option/5"},{"type":"function","title":"Spark.Igniter.update_dsl/5","doc":"","ref":"Spark.Igniter.html#update_dsl/5"},{"type":"module","title":"Spark.InfoGenerator","doc":"Used to dynamically generate configuration functions for Spark extensions\nbased on their DSL.","ref":"Spark.InfoGenerator.html"},{"type":"module","title":"Usage - Spark.InfoGenerator","doc":"```elixir\ndefmodule MyConfig do\n  use Spark.InfoGenerator, extension: MyDslExtension, sections: [:my_section]\nend\n```","ref":"Spark.InfoGenerator.html#module-usage"},{"type":"macro","title":"Spark.InfoGenerator.generate_config_functions/2","doc":"Given an extension and a list of DSL sections generate individual config\nfunctions for each option.","ref":"Spark.InfoGenerator.html#generate_config_functions/2"},{"type":"macro","title":"Spark.InfoGenerator.generate_entity_functions/2","doc":"Given an extension and a list of DSL sections, generate an entities function\nwhich returns a list of entities.","ref":"Spark.InfoGenerator.html#generate_entity_functions/2"},{"type":"macro","title":"Spark.InfoGenerator.generate_options_functions/2","doc":"Given an extension and a list of DSL sections, generate an options function\nwhich returns a map of all configured options for a resource (including\ndefaults).","ref":"Spark.InfoGenerator.html#generate_options_functions/2"},{"type":"function","title":"Spark.InfoGenerator.spec_for_type/2","doc":"","ref":"Spark.InfoGenerator.html#spec_for_type/2"},{"type":"type","title":"Spark.InfoGenerator.options/0","doc":"","ref":"Spark.InfoGenerator.html#t:options/0"},{"type":"extras","title":"Upgrading to 2.0","doc":"# Upgrading to 2.0\n\nA 2.0 release was published with a minor breaking change. We decided to vendor `NimbleOptions` (copy their code into our codebase) so that we could make some necessary modifications to it. What this means for users is primarily that:\n\n1. we no longer depend on `NimbleOptions`\n2. if you are matching on `NimbleOptions.ValidationError` you will need to update your code to match on `Spark.Options.ValidationError`","ref":"upgrade-to-2-0.html"},{"type":"extras","title":"Writing Extensions","doc":"# Writing Extensions\n\nWriting extensions generally involves three main components.","ref":"writing-extensions.html"},{"type":"extras","title":"The DSL declaration - Writing Extensions","doc":"The DSL is declared as a series of `Spark.Dsl.Section`, which can contain `Spark.Dsl.Entity` and further `Spark.Dsl.Section` structs. See `Spark.Dsl.Section` and `Spark.Dsl.Entity` for more information.","ref":"writing-extensions.html#the-dsl-declaration"},{"type":"extras","title":"Transformers - Writing Extensions","doc":"Extension writing gets a bit more complicated when you get into the world of transformers, but this is also where a lot of the power is. Each transformer can declare other transformers it must go before or after, and then is given the opportunity to modify the entirety of the DSL it is extending up to that point. This allows extensions to make rich modifications to the structure in question. See `Spark.Dsl.Transformer` for more information","ref":"writing-extensions.html#transformers"},{"type":"extras","title":"Introspection - Writing Extensions","doc":"Use functions in `Spark.Dsl.Extension` to retrieve the stored values from the DSL and expose them in a module. The convention is to place functions for something like `MyApp.MyExtension` in `MyApp.MyExtension.Info`. Using introspection functions like this allows for a richer introspection API (i.e not just getting and retrieving raw values), and it also allows us to add type specs and documentation, which is helpful when working generically. I.e `module_as_variable.table()` can't be known by dialyzer, whereas `Extension.table(module)` can be.","ref":"writing-extensions.html#introspection"},{"type":"extras","title":"Splitting Up Large DSLs","doc":"# Splitting Up Large DSLs\n\nWhen building large DSLs, we face similar problems as things like large configuration files. It can be hard to find what we're looking for, and we can end up scrolling through a lot of DSL code to find what we're interested in. We generally suggest avoiding splitting up your DSLs by default, but it is important to know how to do so when the need arises.","ref":"split-up-large-dsls.html"},{"type":"extras","title":"Fragments - Splitting Up Large DSLs","doc":"Spark offers a tool called `Spark.Dsl.Fragment`, which allows you to compose a single DSL from multiple smaller DSL modules. There are a few important properties and caveats to understand:\n\n1. Fragments are _not_ designed for sharing code between instances of a spark DSL. They are not dynamic. For creating behavior that extends across multiple instances of a DSL, you should write an extension.\n\n2. A DSL has all extensions that any of its fragments has.\n\n3. Fragments must express what they are a fragment _of_.","ref":"split-up-large-dsls.html#fragments"},{"type":"extras","title":"Example - Splitting Up Large DSLs","doc":"```elixir\ndefmodule MyApp.Accounts.User.Fragments.DataLayer do\n  use Spark.Dsl.Fragment,\n    of: Ash.Resource,\n    data_layer: AshPostgres.DataLayer\n\n  postgres do\n    table \"users\"\n    repo MyApp.Repo\n    ...\n  end\nend\n\ndefmodule MyApp.Accounts.User do\n  use Ash.Resource,\n    fragments: [MyApp.Accounts.User.Fragments.DataLayer]\n\n  ...\nend\n```","ref":"split-up-large-dsls.html#example"},{"type":"extras","title":"Spark","doc":"# Spark\n\nSpark helps you build powerful and well documented DSLs that come with useful tooling out of the box. DSLs are declared using simple structs, and every DSL has the ability to be extended by the end user. Spark powers all of the DSLs in Ash Framework.\n\nWhat you get for your DSL when you implement it with Spark:\n\n- Extensibility. Anyone can write extensions for your DSL.\n- Autocomplete and in-line documentation: An elixir_sense plugin that \"Just Works\" for any DSL implemented with Spark.\n- Tools to generate documentation for your DSL automatically.\n- A mix task to add every part of the DSL to the `locals_without_parens` of your library automatically.\n\nThis library has only recently been extracted out from Ash core, so there is still work to be done to document and test it in isolation. See the module documentation for the most up-to-date information.","ref":"get-started-with-spark.html"},{"type":"extras","title":"Dependency - Spark","doc":"`{:spark, \"~> 2.2.24\"}`","ref":"get-started-with-spark.html#dependency"}],"content_type":"text/markdown","producer":{"name":"ex_doc","version":[48,46,51,52,46,48]}}